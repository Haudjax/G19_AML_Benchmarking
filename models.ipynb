{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c93ff8ca",
   "metadata": {},
   "source": [
    "# Benchmarking Tabular ML Datasets\n",
    "Thom, Jakob and Marit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537691d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2155cc",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3df1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(filename, foldername='aml-2025-benchmarking-tabular-ml-datasets'):\n",
    "    return pd.read_csv(f'{foldername}/{filename}', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba5ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "covtype_test = load_df('covtype_test.csv')\n",
    "covtype_train = load_df('covtype_train.csv')\n",
    "heloc_test = load_df('heloc_test.csv')\n",
    "heloc_train = load_df('heloc_train.csv')\n",
    "higgs_test = load_df('higgs_test.csv')\n",
    "higgs_train = load_df('higgs_train.csv')\n",
    "\n",
    "# Make all target columns have the name 'target'\n",
    "covtype_train.rename(columns={'Cover_Type' : 'label'}, inplace=True)\n",
    "heloc_train.rename(columns={'RiskPerformance' : 'label'}, inplace=True)\n",
    "higgs_train.rename(columns={'Label' : 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948fca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_test = [covtype_test, heloc_test, higgs_test]\n",
    "tables_train = [covtype_train, heloc_train, higgs_train]\n",
    "names = ['CoverType', 'HELOC', 'Higgs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2abceaf",
   "metadata": {},
   "source": [
    "## Combine and clean datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bdfd5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels to convert\n",
    "binary_labels = {\n",
    "    'Bad': 1,\n",
    "    'Good': 0,\n",
    "    's': 1,\n",
    "    'b': 0\n",
    "    }\n",
    "\n",
    "def clean_and_combine(tables, names, binary_labels = None):\n",
    "    \n",
    "    cleaned_tables = []\n",
    "\n",
    "    for table, name in zip(tables, names):\n",
    "        t = table.copy()\n",
    "\n",
    "        # Get numerical columns for this specific table\n",
    "        numerical_cols = t.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "        # Clean missing values based on domain and remove id column\n",
    "        if name == 'HELOC':\n",
    "            for col in numerical_cols:\n",
    "                t.loc[t[col] < 0, col] = np.nan\n",
    "        elif name == 'HIGGS':\n",
    "            t.replace(-999.0, np.nan, inplace=True)\n",
    "            t = t.drop('EventId')\n",
    "\n",
    "        # Add domain name\n",
    "        t['Domain'] = name\n",
    "        cleaned_tables.append(t)\n",
    "        \n",
    "    unified_df = pd.concat(cleaned_tables, ignore_index=True)\n",
    "\n",
    "    # Handle target labels if provided (Training Data)\n",
    "    if binary_labels:\n",
    "        unified_df['label'] = unified_df['label'].astype(str).replace(binary_labels)       # As string first to prevent downcasting warning\n",
    "        unified_df['label'] = unified_df['label'].astype(int)\n",
    "    return unified_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eff1e0",
   "metadata": {},
   "source": [
    "## These are some different models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c120fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# catboost = Pipeline([\n",
    "#     (\"preprocessor\", unified_preprocessor),\n",
    "#     (\"clf\", CatBoostClassifier(\n",
    "#         iterations=1000, \n",
    "#         learning_rate=0.05,\n",
    "#         depth=6,             # Start shallower than XGBoost\n",
    "#         loss_function='MultiClass',\n",
    "#         verbose=0,\n",
    "#         random_state=42,\n",
    "#         auto_class_weights='Balanced' # Critical for CoverType imbalance\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# xgboost = Pipeline([\n",
    "#     (\"preprocessor\", unified_preprocessor),\n",
    "#     (\"clf\", XGBClassifier(\n",
    "#         n_estimators=500,\n",
    "#         learning_rate=0.05,\n",
    "#         max_depth=10,\n",
    "#         tree_method='hist',\n",
    "#         n_jobs=-1,\n",
    "#         random_state=42,\n",
    "#     ))\n",
    "#     ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lightxgboost = Pipeline([\n",
    "#         (\"preprocessor\", unified_preprocessor),\n",
    "#         (\"clf\", LGBMClassifier(\n",
    "#             n_estimators=300,\n",
    "#             learning_rate=0.05,\n",
    "#             num_leaves=31,\n",
    "#             # fix for imbalance in CoverType classes\n",
    "#             class_weight='balanced', \n",
    "#             random_state=random_state,\n",
    "#             n_jobs=-1\n",
    "#         ))\n",
    "#     ])\n",
    "\n",
    "\n",
    "\n",
    "# mlp  = Pipeline([\n",
    "#     (\"preprocessor\", unified_preprocessor),\n",
    "#     (\"clf\", MLPClassifier(\n",
    "#         hidden_layer_sizes=(256, 128, 64), # 3 deep layers (Heavy!)\n",
    "#         activation='relu',\n",
    "#         solver='adam',\n",
    "#         batch_size=256,\n",
    "#         max_iter=200,            # Epochs\n",
    "#         early_stopping=True,     # Stop if validation score stops improving\n",
    "#         random_state=42\n",
    "#     ))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6b135",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## This is to run one model with single set of parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "359e09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lockandload(df, random_state=42, test_size=0.2, target_col = 'label'):\n",
    "\n",
    "#     # Separate features and target\n",
    "#     X = df.drop(target_col, axis=1)\n",
    "#     y = df[target_col]\n",
    "\n",
    "#     # Get numerical columns for transformer\n",
    "#     numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "#     categorical_cols = ['Domain']\n",
    "\n",
    "#     # Pre processing pipiline\n",
    "#     unified_preprocessor = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             ('num_processing', Pipeline([\n",
    "#                 # replace NaN's and use scaler\n",
    "#                 ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "#                 ('scaler', StandardScaler())\n",
    "#             ]), numerical_cols),\n",
    "            \n",
    "#             ('cat_processing', Pipeline([\n",
    "#                 # One hot encode the tabular name (Domain)\n",
    "#                 ('onehot', OneHotEncoder())\n",
    "#             ]), categorical_cols)\n",
    "#         ],\n",
    "#         remainder='drop' \n",
    "#     )\n",
    "\n",
    "\n",
    "#     unified_pipeline  = Pipeline([\n",
    "#         (\"preprocessor\", unified_preprocessor),\n",
    "#         (\"clf\", LGBMClassifier(\n",
    "#             n_estimators=300,\n",
    "#             learning_rate=0.05,\n",
    "#             num_leaves=31,\n",
    "#             # fix for imbalance in CoverType classes\n",
    "#             class_weight='balanced', \n",
    "#             random_state=random_state,\n",
    "#             n_jobs=-1\n",
    "#         ))\n",
    "#     ])\n",
    "\n",
    "#     # Train / test split\n",
    "#     X_train, X_test, y_train_enc, y_test_enc = train_test_split(\n",
    "#         X, y,\n",
    "#         test_size=test_size,\n",
    "#         stratify=y,\n",
    "#         random_state=random_state\n",
    "#     )\n",
    "\n",
    "\n",
    "#     print(\"Training unified model\")\n",
    "#     model = (unified_pipeline)\n",
    "#     model.fit(X_train, y_train_enc)\n",
    "\n",
    "#     # Evaluate on local test set\n",
    "#     y_pred_enc = model.predict(X_test)\n",
    "#     acc = accuracy_score(y_test_enc, y_pred_enc)\n",
    "\n",
    "#     print(f\"\\nLocal test accuracy: {acc:.4f}\")\n",
    "\n",
    "#     return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf08a47",
   "metadata": {},
   "source": [
    "## Parameter search on lightxgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b79406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightxg_params(df, random_state=42, test_size=0.2, target_col='label'):\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Get numerical columns\n",
    "    numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    if 'ID' in numerical_cols: numerical_cols.remove('ID') # Safety check\n",
    "        \n",
    "    categorical_cols = ['Domain']\n",
    "\n",
    "    # 3. Define Preprocessor\n",
    "    unified_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num_processing', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numerical_cols),\n",
    "            \n",
    "            ('cat_processing', Pipeline([\n",
    "                # handle_unknown='ignore' is crucial for robustness\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=int))\n",
    "            ]), categorical_cols)\n",
    "        ],\n",
    "        remainder='drop' \n",
    "    )\n",
    "\n",
    "    # Define Base Pipeline \n",
    "    unified_pipeline = Pipeline([\n",
    "        (\"preprocessor\", unified_preprocessor),\n",
    "        (\"clf\", LGBMClassifier(\n",
    "            class_weight='balanced', # Keep this fixed\n",
    "            random_state=random_state,\n",
    "            n_jobs=1,\n",
    "            verbose=-1 # Silence LGBM internal logs\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Define Search Space\n",
    "    param_dist = {\n",
    "        'clf__n_estimators': randint(100, 1000),      # Number of trees\n",
    "        'clf__learning_rate': uniform(0.01, 0.2),     # Learning speed\n",
    "        'clf__num_leaves': randint(20, 150),          # Tree complexity\n",
    "        'clf__max_depth': randint(5, 20),             # Depth limit\n",
    "        'clf__min_child_samples': randint(10, 100),   # Regularization\n",
    "        'clf__subsample': uniform(0.6, 0.4),          # Bagging fraction\n",
    "        'clf__colsample_bytree': uniform(0.6, 0.4)    # Feature fraction\n",
    "    }\n",
    "\n",
    "    # Train / Test Split\n",
    "    X_train, X_test, y_train_enc, y_test_enc = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Configure Randomized Search\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=unified_pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=15,            # Try 15 different random combinations\n",
    "        cv=3,                 # 3-Fold Cross Validation\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,            # Use all CPU cores\n",
    "        verbose=1,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(\"Starting Hyperparameter Tuning (RandomizedSearchCV)...\")\n",
    "    search.fit(X_train, y_train_enc)\n",
    "\n",
    "    # Get Best Model & Results\n",
    "    best_model = search.best_estimator_\n",
    "    print(f\"\\nBest Validation Accuracy (CV): {search.best_score_:.4f}\")\n",
    "    print(f\"Best Parameters: {search.best_params_}\")\n",
    "\n",
    "    # Evaluate on Local Test Set\n",
    "    y_pred_enc = best_model.predict(X_test)\n",
    "    acc = accuracy_score(y_test_enc, y_pred_enc)\n",
    "\n",
    "    print(f\"Local Test Set Accuracy: {acc:.4f}\")\n",
    "\n",
    "    os.system('say \"Your unified model training is finished.\"')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fb46aa",
   "metadata": {},
   "source": [
    "## Parameter search on Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46091e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_params(df, random_state=42, test_size=0.2, target_col='label'):\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Get numerical columns\n",
    "    numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    if 'ID' in numerical_cols: numerical_cols.remove('ID')\n",
    "        \n",
    "    categorical_cols = ['Domain']\n",
    "\n",
    "    # Define preprocessor \n",
    "    unified_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num_processing', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numerical_cols),\n",
    "            \n",
    "            ('cat_processing', Pipeline([\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=int))\n",
    "            ]), categorical_cols)\n",
    "        ],\n",
    "        remainder='drop' \n",
    "    )\n",
    "\n",
    "    # Pipeline with Catboost\n",
    "    unified_pipeline = Pipeline([\n",
    "        (\"preprocessor\", unified_preprocessor),\n",
    "        (\"clf\", CatBoostClassifier(\n",
    "            loss_function='MultiClass',   # Required for your 11 classes\n",
    "            auto_class_weights='Balanced',# Handles the CoverType imbalance automatically\n",
    "            verbose=0,                    # Silence the training output\n",
    "            random_state=random_state,\n",
    "            thread_count=-1,              # Use all cores for the model training\n",
    "            allow_writing_files=False     # Prevents creating 'catboost_info' folders\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Parameters to search\n",
    "    param_dist = {\n",
    "        'clf__iterations': randint(500, 1200),\n",
    "        'clf__depth': randint(4, 9), # Keep max 8!\n",
    "        'clf__learning_rate': uniform(0.01, 0.2),\n",
    "        'clf__l2_leaf_reg': randint(1, 10),\n",
    "        'clf__bagging_temperature': uniform(0, 1),\n",
    "        'clf__random_strength': uniform(1, 10)\n",
    "    }\n",
    "\n",
    "    # Train / Test Split\n",
    "    X_train, X_test, y_train_enc, y_test_enc = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Configure Randomized Search\n",
    "    # IMPORTANT: n_iter=10 is enough for CatBoost because it trains slower\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=unified_pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,            # Lower than LGBM due to speed\n",
    "        cv=3,                 \n",
    "        scoring='accuracy',\n",
    "        n_jobs=1,             # Sequential search to avoid thread explosion\n",
    "        verbose=1,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(\"Starting CatBoost Hyperparameter Tuning...\")\n",
    "    search.fit(X_train, y_train_enc)\n",
    "\n",
    "    # Get Best Model & Results\n",
    "    best_model = search.best_estimator_\n",
    "    print(f\"\\nBest Validation Accuracy (CV): {search.best_score_:.4f}\")\n",
    "    print(f\"Best Parameters: {search.best_params_}\")\n",
    "\n",
    "    # Evaluate on Local Test Set\n",
    "    y_pred_enc = best_model.predict(X_test)\n",
    "    # CatBoost predict returns an array of shape (N, 1), flatten it\n",
    "    y_pred_enc = y_pred_enc.ravel() \n",
    "    \n",
    "    acc = accuracy_score(y_test_enc, y_pred_enc)\n",
    "\n",
    "    print(f\"Local Test Set Accuracy: {acc:.4f}\")\n",
    "\n",
    "    os.system('say \"Your unified model training is finished.\"')\n",
    "\n",
    "    # Return the TUNED model so you can use it for submission\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0afe434",
   "metadata": {},
   "source": [
    "## Parameter search on MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f207dc",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8985a48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CatBoost Hyperparameter Tuning...\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "\n",
      "Best Validation Accuracy (CV): 0.9430\n",
      "Best Parameters: {'clf__activation': 'tanh', 'clf__alpha': np.float64(0.008424426408004218), 'clf__batch_size': 512, 'clf__hidden_layer_sizes': (256, 128), 'clf__learning_rate_init': np.float64(0.002818249672071006), 'clf__solver': 'adam'}\n",
      "Local Test Set Accuracy: 0.9472\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df_train = clean_and_combine(tables_train, names, binary_labels)\n",
    "df_test = clean_and_combine(tables_test, names)\n",
    "\n",
    "# Train model\n",
    "model = mlp_params(df_train)\n",
    "\n",
    "# Predict on submission test set\n",
    "predictions = model.predict(df_test)\n",
    "\n",
    "# Save submission file\n",
    "submission = pd.DataFrame({\"Prediction\": predictions}, index=range(1, len(predictions) + 1))\n",
    "\n",
    "### Only necessary for catboost, output data is different shape ###\n",
    "# submission = pd.DataFrame({\"Prediction\": predictions.ravel()}, index=range(1, len(predictions) + 1))\n",
    "\n",
    "pd.DataFrame(submission).to_csv('combined_test_sample_submission.csv', index=True, index_label=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba285a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
